---
format:
    revealjs:
      slide-number: false
      progress: false
      chalkboard: false
      code-overflow: wrap
      theme: [default, custom.scss]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE,
                      message = FALSE,
                      fig.pos = "center")

library(tidyverse)
library(tinytable)
library(kableExtra)
library(gt)
library(modelsummary)
library(marginaleffects)
library(tidyr)
library(broom)
library(haven)
library(patchwork)


# ggplot global options
theme_set(theme_gray(base_size = 20))

# set NU purple
nu_purple = "#4E2A84"
```



::: {style="text-align: center"}
## Machine Learning {{< fa robot >}} {.center}

### POLI SCI 210

Introduction to Empirical Methods in Political Science

:::



## Roadmap

- **Tuesday:** Big picture, simple models

- [**Thursday:**](#thu) Fancier models, generative AI

## Summary of the course

- Focus on **inference** since it is how political scientists test theories

- **Statistical inference:** summarize data, quantify uncertainty

- **Univariate:** Mean, confidence intervals, standard errors

- **Bivariate:** Difference in means (experiments, potential outcomes)

- **Multivariate:** OLS regression

## Summary of the course

- **Subplot:** *Bivariate* and *multivariate* only make sense if we want to make **causal statements**

- **Causal inference:** Impose some structure to justify assumptions

- **Small N:** Necessary and sufficient as logic for inference

. . .

But *inference* is not the only thing we do with *data*

---

![](https://workshops.tidymodels.org/slides/images/what_is_ml.png)

## How are they different?

```{=html}
<p align="center"><iframe width="900" height="500" src="https://www.youtube.com/embed/uHGlCi9jOWY?si=nhKlbLuU6Zd9sK1u" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
```

::: aside
<https://youtu.be/uHGlCi9jOWY?si=wgfvS9IiV5_FQ2aT>
:::

## Ok, but how are they different?

. . .

**Statistical inference**

::: incremental
- Use data we have to learn about a target population

- Or measure a quantify of interest

- *Main product:* Estimates, uncertainty
:::

**Statistical learning**

::: incremental
- Use data we have to *predict* how new data will look like

- Minimize *prediction error*

- *Main product:* Rules, error metrics
:::

## Same stuff, different language

```{r}
variables = tribble(
  ~`Statistical inference`, ~`Statistical learning`,
  "Outcome variable", "Response, output",
  "Explanatory variable", "Predictor, input, feature",
  "Model", "Algorithm",
  "Uncertainty", "Error"
)

variables %>% tt() %>% 
  style_tt(
    i = 1:4,
    color = "#00000000"
  )
```

## Same stuff, different language

```{r}
variables %>% tt() %>% 
    style_tt(
    i = 2:4,
    color = "#00000000"
  )
```

## Same stuff, different language

```{r}
variables %>% tt() %>% 
    style_tt(
    i = 3:4,
    color = "#00000000"
  )
```

## Same stuff, different language

```{r}
variables %>% tt() %>% 
    style_tt(
    i = 4,
    color = "#00000000"
  )
```

## Same stuff, different language

```{r}
variables %>% tt()
```

## Flavors of machine learning

![](https://workshops.tidymodels.org/slides/images/ml_illustration.jpg){fig-align="center"}



## Toy example

```{r cookies-base, fig.align="center"}
cookies <- tibble(happiness = c(0.5, 2, 1, 2.5, 3, 1.5, 2, 2.5, 2, 3),
                  cookies = 1:10)

cookies_data <- cookies
cookies_model <- lm(happiness ~ cookies, data = cookies)
cookies_fitted <- augment(cookies_model)

cookies_base <- ggplot(cookies_fitted, aes(x = cookies, y = happiness)) +
  geom_point(size = 3) +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 3)) +
  scale_x_continuous(breaks = 0:10) +
  labs(x = "Cookies eaten", y = "Level of happiness")

cookies_base
```

. . .

How happy will the next person be?

## They eat 5 cookies

```{r, fig.align='center'}
cookies_base +
  geom_vline(xintercept = 5, 
             linewidth = 2,
             linetype = "dashed", 
             color = nu_purple)
```

How happy will the next person be?

## They eat 5 cookies

```{r, fig.align='center'}
cookies_base +
  geom_vline(xintercept = 5, 
             linewidth = 2,
             linetype = "dashed", 
             color = nu_purple)
```

We already know one way

## Drawing lines!

```{r cookies-spline}
c1 = cookies_base +
  geom_smooth(method = lm, color = nu_purple, formula = y ~ splines::bs(x, 7), se = FALSE)

c1
```

## Drawing lines!

```{r cookies-loess}
c2 = cookies_base +
  geom_smooth(method = "loess", color = nu_purple, se = FALSE)

c2
```

## Drawing lines!

```{r cookies-lm}
c3 =cookies_base +
  geom_smooth(method = "lm", color = nu_purple, se = FALSE)

c3
```

## Which one seems better?

```{r}
c1 + c2 + c3 + plot_layout(axes = "collect")
```

## Need to balance

::: incremental

1. Being as close as possible

2. Avoid relying on specific observations
:::

## We already have language for this

![](fig/bias_variance.jpg){fig-align="center"}

---

```{r}
c1 + labs(subtitle = "Overfitting") +
  c2 + labs(subtitle = "Compromise") +
  c3 + labs(subtitle = "Underfitting") +
  plot_layout(axes = "collect")
```

. . .

**Overfitting:** Low bias, high variance

. . .

**Underfitting:** High bias, low variance

## Supervised learning methods

. . .

**Parametric:** Functional form can be written as a linear combination (e.g. OLS)

. . .

**Nonparametric:** Cannot be written as a linear combination


## Nonparametric cookies

```{r, fig.align='center'}
cookies_base +
  geom_vline(xintercept = 5, 
             linewidth = 2,
             linetype = "dashed", 
             color = nu_purple)
```

What would be a good guess for the new person?

## Nonparametric cookies

```{r, fig.align='center'}
cookies <- tibble(happiness = c(0.5, 2, 1, 2.5, 3, 1.5, 2, 2.5, 2, 3),
                  cookies = 1:10)

cookies$nb = c(0, 0, 0, 1, 1, 1, 0, 0, 0, 0)

cookies_nb = ggplot(cookies) +
  aes(x = cookies, 
      y = happiness,
      color = as.factor(nb)) +
      geom_vline(xintercept = 5, 
             linewidth = 2,
             linetype = "dashed", 
             color = nu_purple) +
  geom_point(size = 3) +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 3)) +
  scale_x_continuous(breaks = 0:10) +
  scale_color_manual(values = c("black", "green")) +
  labs(x = "Cookies eaten", y = "Level of happiness") +
  theme(legend.position = "none")
  
cookies_nb
```

Observations nearby!

## K-Nearest Neighbors (KNN)

![](fig/knn.png)

## KNN algorithm

For each new observation:

::: incremental
- Find K closest observations based on observed features

- **Regression:** Predict new value taking the average of all neighbors

- **Classification:** Predict category with highest probability among neighbors
:::

. . .

**Pros:** Flexible. Works better than what you would think

. . .

**Cons:** Computationally inefficient, struggles with complex data structures

## Classification example

```{r, fig.align='center'}
library(ISLR)

Default

Default$default01 = ifelse(Default$default == "Yes", 1, 0)

ols = lm(default01 ~ student + balance + income, data = Default)

logit = glm(default01 ~ student + balance + income, 
            data = Default, family = binomial)
```

. . .

**Goal:** *predict* whether a customer's credit card will go on default

## Visualize

```{r}
p_ols = plot_predictions(ols,
                 condition = "balance",
                 draw = FALSE)

p_log = plot_predictions(logit,
                         condition = "balance",
                         draw = FALSE)

# Combine and label
pred_df = bind_rows(
  p_ols %>% mutate(Model = "OLS"),
  p_log %>% mutate(Model = "Logit")
)

# Visualize
ggplot(Default) +
  aes(x = balance, y = default01) +
  geom_jitter(alpha = 0.3, height = 0.1) +
  geom_line(
    data = pred_df,
    alpha = 0,
    aes(x = balance, 
        y = estimate,
        color = Model),
    linewidth = 2) +
  scale_y_continuous(limits = c(0,1)) +
  scale_color_viridis_d(begin = 0, end = 0.8) +
  labs(x = "Balance",
       y = "Pr(Default)") + 
  theme(legend.position = "none")
```

. . .

Tricky to find neighbors

## Visualize

```{r}
# Visualize
ggplot(Default) +
  aes(x = balance, y = default01) +
  geom_jitter(alpha = 0.3, height = 0.1) +
  geom_line(
    data = pred_df,
    aes(x = balance, 
        y = estimate,
        color = Model),
    linewidth = 2) +
  scale_y_continuous(limits = c(0,1)) +
  scale_color_viridis_d(begin = 0, end = 0.8, 
                        alpha = c(0, 1)) +
  labs(x = "Balance",
       y = "Pr(Default)") + 
  theme(legend.position = "none")
```

What about OLS regression?

## Predicted probabilities

```{r}
# Visualize
ggplot(Default) +
  aes(x = balance, y = default01) +
  geom_jitter(alpha = 0.3, height = 0.1) +
  geom_line(
    data = pred_df,
    aes(x = balance, 
        y = estimate,
        color = Model),
    linewidth = 2) +
  scale_y_continuous(limits = c(0,1)) +
  scale_color_viridis_d(begin = 0, end = 0.8, 
                        alpha = c(0, 1)) +
  labs(x = "Balance",
       y = "Pr(Default)") + 
  theme(legend.position = "none")
```

Not that good at catching those who may default

## Predicted probabilities

```{r}
# Visualize
ggplot(Default) +
  aes(x = balance, y = default01) +
  geom_jitter(alpha = 0.3, height = 0.1) +
  geom_line(
    data = pred_df,
    aes(x = balance, 
        y = estimate,
        color = Model),
    linewidth = 2) +
  scale_y_continuous(limits = c(0,1)) +
  scale_color_viridis_d(begin = 0, end = 0.8, 
                        alpha = c(0, 1)) +
  labs(x = "Balance",
       y = "Pr(Default)") + 
  theme(legend.position = "none")
```

Also it can (technically) exceed the 0-1 range!

## What is the problem?

. . .

**Before:** We wanted a single number summary that characterizes the relationship and has good statistical properties

. . .

**Now:** We want a model that predicts new data well, we don't care about producing precise or interpretable estimates

. . .

We also want a model that produces **valid** classifications!

## Logistic regression

::: incremental
- A variant of regression that respects the laws of probability

- Uses an intermediate step called a **link function**
:::

## Logistic regression


<!-- My old cheatsheet -->
<!-- https://drive.google.com/drive/folders/1y0SwNYAjG5_uDGj7MaaJJoS3TpZ_5HuL -->

For the logit model, the link is the *logistic function*

$$
p(X) = \frac{e^{X\beta}}{1+e^{X\beta}}
$$

. . .


```{r, echo = FALSE, fig.width=5, fig.height=4, fig.align='center'}
library(tidyverse)

ggplot(data = data.frame(x = c(-3,3))) +
  aes(x) +
  stat_function(fun = function(x) exp(x)/(1+exp(x)), n = 100, linewidth = 2) +
  ylim(0,1) +
  labs(y = "p(X)")
  
```


## Logistic regression


For the logit model, the link is the *logistic function*

$$
p(X) = \frac{e^{X\beta}}{1+e^{X\beta}}
$$

. . .

Rearrange to get the *odds ratio*

$$
\frac{p(X)}{1-p(X)} = e^{{X\beta}}
$$

## Logistic regression


Taking the natural logarithm gives the *log odds*

$$
log \left (\frac{p(X)}{1-p(X)} \right) = X\beta
$$

## Logistic regression


Taking the natural logarithm gives the *log odds*

$$
log \left (\frac{p(X)}{1-p(X)} \right) = \beta_0 + \beta_1x_1 + \beta_2 x_2 + \ldots + \beta_k x_k + \varepsilon
$$

::: incremental
- It's called *logit* because you need to *log* *it* to compute
- Estimate with **maximum likelihood estimation**
- Weird to interpret, but we do not care as long as we get good *classification*
:::

## What changes?

```{r}
# Visualize
ggplot(Default) +
  aes(x = balance, y = default01) +
  geom_jitter(alpha = 0.3, height = 0.1) +
  geom_line(
    data = pred_df,
    aes(x = balance, 
        y = estimate,
        color = Model),
    linewidth = 2) +
  scale_y_continuous(limits = c(0,1)) +
  scale_color_viridis_d(begin = 0, end = 0.8, 
                        alpha = c(0, 1)) +
  labs(x = "Balance",
       y = "Pr(Default)")
```

## What changes?

```{r}
# Visualize
ggplot(Default) +
  aes(x = balance, y = default01) +
  geom_jitter(alpha = 0.3, height = 0.1) +
  geom_line(
    data = pred_df,
    aes(x = balance, 
        y = estimate,
        color = Model),
    linewidth = 2) +
  scale_y_continuous(limits = c(0,1)) +
  scale_color_viridis_d(begin = 0, end = 0.8) +
  labs(x = "Balance",
       y = "Pr(Default)")
```

## Great! What next?

. . .

We have baby's first machine learning models

. . .

How do we now if these (or any other fancy model) performs well?

. . .

We need a way to *quantify* **prediction error**

## Error metrics

**Regression**

. . .

Remember this?

$$
SSR = \sum_{i=1}^n e_i^2
$$

## Error metrics

**Regression**

Remember this?

$$
SSR = \sum_{i=1}^n (y_i - \widehat y_i)^2
$$

. . . 

It's our friend the **Sum of Squared Residuals**!

. . .

You used to be a *criterion* to minimize so that we could draw good lines

. . .

Now you are an **error metric**

## Error metrics

**Regression**

$$
SSR = \sum_{i=1}^n (y_i - \widehat y_i)^2
$$

But not with those clothes!

## Error metrics

**Regression**

$$
MSE = \frac{\sum_{i=1}^n (y_i - \widehat y_i)^2}{N}
$$

But not with those clothes!

. . .

Now you are a **Mean Squared Error**

. . .

But you could look prettier!

## Error metrics

**Regression**

$$
RMSE = \sqrt{\frac{\sum_{i=1}^n (y_i - \widehat y_i)^2}{N}} 
$$

. . .

You are a **Root Mean Squared Error**

. . .

You are now expressed on *response variable* units {{< fa heart >}}

. . .

{{< fa arrow-down >}} RMSE $\Rightarrow$ Better prediction


## Error metrics

**Classification**

```{r}
confusion = tribble(
  ~Predicted, ~`False (0)`, ~`True (1)`,
  "False (0)", "True Negative (**TN**)", "False Negative (**FN**)",
  "True (1)", "False Positive (**FP**)", "True Positive (**TP**)"
  
)

confusion %>% tt() %>% 
  format_tt(j = 2:3, markdown = TRUE) %>% 
  group_tt(
    j = list(
      "Actual" = 2:3
    )
  )
```

. . .

We can use these to calculate several metrics

## Error metrics

**Classification**

```{r, results='asis'}
class_metrics = tribble(
  ~Name, ~Measurement, ~Note,
  "Error rate", "$Avg(I(y_i \\neq \\widehat{y}_i))$",
  "Proportion actual $\\neq$ predicted",
  "Accuracy", "$1 - \\text{error rate}$", "Proportion correct",
  "Accuracy", "$(TN + TP)/n$", "Proportion correct",
  "Sensitivity", "$TP/(TP+FN)$", "Proportion correct positives",
  "Specificity", "$TN/(TN+FP)$", "Proportion correct negatives"
  
  
)


tab0 = class_metrics %>% 
  kbl() %>% 
  kable_styling(font_size = 35) %>% 
  row_spec(1:5, color = "#00000000")

cat(tab0)
```


## Error metrics

**Classification**

```{r, results='asis'}
tab1 = class_metrics %>% 
  kbl() %>% 
  kable_styling(font_size = 35) %>% 
  row_spec(2:5, color = "#00000000")

cat(tab1)
```

## Error metrics

**Classification**

```{r, results='asis'}
tab2 = class_metrics %>% 
  kbl() %>% 
  kable_styling(font_size = 35) %>% 
  row_spec(3:5, color = "#00000000")

cat(tab2)
```



## Error metrics

**Classification**

```{r, results='asis'}
tab3 = class_metrics %>% 
  kbl() %>% 
  kable_styling(font_size = 35) %>% 
  row_spec(4:5, color = "#00000000")

cat(tab3)
```

## Error metrics

**Classification**

```{r, results='asis'}
tab4 = class_metrics %>% 
  kbl() %>% 
  kable_styling(font_size = 35) %>% 
  row_spec(5, color = "#00000000")

cat(tab4)
```

## Error metrics

**Classification**

```{r, results='asis'}
tab5 = class_metrics %>% 
  kbl() %>% 
  kable_styling(font_size = 35)

cat(tab5)
```

## Hold on

Aren't these metrics assuming that we **know** true positives/negatives?

. . .

How do we calculate if we don't know?

. . .

More next time!

::: {style="text-align: center"}
## Machine Learning {{< fa robot >}} {#thu .center}

### POLI SCI 210

Introduction to Empirical Methods in Political Science

:::

## Last time

**Error metrics in machine learning**

**Regression:** Root Mean Squared Error (RMSE)

**Classification:** Error rate, accuracy, sensitivity, specificity

. . .

These require *actual* and *predicted* values

. . .

But why predict if you know *actual* values?

&nbsp;

. . .

**Remember:** We are doing this to learn about *new data*

## Resampling methods

. . .

**General idea:** Use existing data to mimic predicting new data

. . .

**Easiest:** Validation set approach

::: incremental
- Split data into **training** and **validation** set
- Normally via random sampling
- Usually larger training set
- Generate predictions on training set
- Evaluate performance on validation set
:::

## Example: Auto data

```{r}
ggplot(Auto) +
  aes(x = horsepower, y = mpg) +
  geom_point(size = 2, color = nu_purple)
```

## OLS models

**Linear:** $\widehat{\texttt{mpg}} = \beta_0 + \beta_1 \texttt{horsepower}$

. . .

**Quadratic:** $\widehat{\texttt{mpg}} = \beta_0 + \beta_1 \texttt{horsepower} + \beta_2 \texttt{horsepower}^2$

. . .

**Cubic:** $\widehat{\texttt{mpg}} = \beta_0 + \beta_1 \texttt{horsepower} + \beta_2 \texttt{horsepower}^2 + \beta_3 \texttt{horsepower}^3$

&nbsp;

. . .

More polynomial terms $\rightarrow$ more curvy

. . .

50/50 train/validation split at random

. . .

Choose model that would predict new data better

## Results

```{r}
# split
set.seed(1)
train1 = sample(392 ,196)

set.seed(2)
train2 = sample(392 ,196)

# fit
fit1 = lm(mpg ~ horsepower, data=Auto ,subset = train1)

fit2 = lm(mpg ~ poly(horsepower, 2), data=Auto ,subset = train1)

fit3 = lm(mpg ~ poly(horsepower, 3), data=Auto ,subset = train1)

# rmse
linear = sqrt(mean((Auto$mpg - predict(fit1, Auto))[-train1]^2))

quad = sqrt(mean((Auto$mpg - predict(fit2, Auto))[-train1]^2))

cubic = sqrt(mean((Auto$mpg - predict(fit3, Auto))[-train1]^2))
```


```{r}
# make table
auto_lm = data.frame(
  Fit = c("Linear", "Quadratic", "Cubic"),
  RMSE = c(linear, quad, cubic)
)

auto_lm %>% kbl(digits = 2)
```


. . .

Notice how results change based on train/validation split

## Results

```{r}
# fit
fit1b = lm(mpg ~ horsepower, data=Auto ,subset = train2)

fit2b = lm(mpg ~ poly(horsepower, 2), data=Auto ,subset = train2)

fit3b = lm(mpg ~ poly(horsepower, 3), data=Auto ,subset = train2)

# rmse
linearb = sqrt(mean((Auto$mpg - predict(fit1, Auto))[-train2]^2))

quadb = sqrt(mean((Auto$mpg - predict(fit2, Auto))[-train2]^2))

cubicb = sqrt(mean((Auto$mpg - predict(fit3, Auto))[-train2]^2))

auto_lm$RMSEb = c(linearb, quadb, cubicb)

colnames(auto_lm) = c("Fit", "Split 1", "Split 2")
```

```{r}
auto_lm %>% 
  kbl(digits = 2) %>% 
  add_header_above(c(" " = 1, "RMSE" = 2))
```

. . .

Fancier *resampling methods* take advantage of this to provide more robust performance

. . .

**Cost:** Increased computing times (but trivial for consumer-level tasks)

## Example: Cross-validation

**Idea:** Do many train-validation splits and then average over their performance

## Example: Cross-validation

**L**eave-**O**ne-**O**ut **C**ross-**V**alidation (LOOCV)

. . .

![](fig/cv_loo.png)

## Example: Cross-validation

_**K-fold**_ cross-validation

. . .

![](fig/cv_k.png)

## Application: credit cards data

```{r}
Default %>% select(!default01)
```

. . .

**Goal:** Predict who will default on their credit card

## Algorithms

**Logistic regression:**

$$
\widehat p(\texttt{default}) = \beta_0 + \beta_1 \texttt{income} + \beta_2 \texttt{balance} + \beta_3 \texttt{student}
$$

. . .

Compare with **KNN** (5, 10, 20)

. . .

All tuned with *5-fold CV*

## Results

```{r, cache = TRUE}
library(caret)

Default$default = fct_rev(Default$default)

set.seed(20250303)

logit_cv_acc = train(
  form = default ~ income + balance + student,
  data = Default,
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 5)
  
)

logit_cv_roc = train(
  form = default ~ income + balance + student,
  data = Default,
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 5,
                           classProbs=TRUE,
                           summaryFunction=twoClassSummary)
)

knn_grid = expand.grid(k = c(5, 10, 20))

knn_cv_acc = train(
  form = default ~ income + balance + student,
  data = Default, 
  method = "knn",
  trControl = trainControl(method = "cv", number = 5),
  tuneGrid = knn_grid
)

knn_cv_roc = train(
  form = default ~ income + balance + student,
  data = Default, 
  method = "knn",
  trControl = trainControl(method = "cv", number = 5,
                           classProbs=TRUE,
                           summaryFunction=twoClassSummary),
  tuneGrid = knn_grid
)

logit_cv = bind_cols(logit_cv_acc$results,
                     logit_cv_roc$results)

knn_cv = bind_cols(knn_cv_acc$results,
                   knn_cv_roc$results)


default_cv = bind_rows(
  logit_cv %>% mutate(Algorithm = "Logit"),
  knn_cv %>% mutate(Algorithm = "KNN")
) %>% 
  select(Algorithm,
         k = k...1, 
         Accuracy, 
         Sensitivity = Sens, 
         Specificity = Spec)
```

```{r}
default_cv %>% 
  tt(digits = 2) %>% 
  style_tt(
    j = 3:5,
    color = "#00000000"
  )
```

## Results

```{r}
default_cv %>% 
  tt(digits = 2) %>% 
  style_tt(
    j = 4:5,
    color = "#00000000"
  )
```

## Results

```{r}
default_cv %>% 
  tt(digits = 2) %>% 
  style_tt(
    j = 5,
    color = "#00000000"
  )
```

## Results

```{r}
default_cv %>% 
  tt(digits = 2)
```

. . .

Which one seems more appropriate?

## Fancier models

::: incremental

- Achieve even higher performance at the expense of even more computing power

- Can be **parametric** or **nonparametric**

- Technically, they all have *tuning parameters*

- **Difference:** Functional form assumptions
:::

. . .

These are a few examples at the limits of consumer-level computing power

## Regression/classification trees

![](fig/trees.jpg){fig-align="center"}

## Algorithm: Recursive binary partitioning

![](fig/recursive_1.jpg){fig-align="center"}

## Algorithm: Recursive binary partitioning

![](fig/recursive_2.jpg){fig-align="center"}

## Algorithm: Recursive binary partitioning

![](fig/recursive_3.jpg){fig-align="center"}

## Algorithm: Recursive binary partitioning

![](fig/recursive_4.jpg){fig-align="center"}

## Algorithm: Recursive binary partitioning

![](fig/recursive_5.jpg){fig-align="center"}

## Algorithm: Recursive binary partitioning

![](fig/recursive_6.jpg){fig-align="center"}

## Random forests

![](fig/random_forest.jpg){fig-align="center"}

## Neural networks

```{=html}
<p align="center"><iframe width="900" height="500" src="https://www.youtube.com/embed/bfmFfD2RIcg?si=xAEVyJ3BKr2JCQzF" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>

```

<https://youtu.be/bfmFfD2RIcg?si=xAEVyJ3BKr2JCQzF>

## Beyond consumer-level

. . .

Mostly **deep learning** models trained on vast amounts of **unstructured** data, then used to create *new* data

. . . 

Combination of **extractive** and **generative** AI

. . .

- **Extractive:** Learns patterns, gives structure (supervised/unsupervised learning)

- **Generative:** Creates new information

. . .

The magic behind *generative AI* as it exists today is the **transformer architecture**

## Transformer architecture

```{=html}
<p align="center"><iframe width="900" height="500" src="https://www.youtube.com/embed/ZXiruGOCn9s?si=QWnI4-mQDc_SnoHB" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
```


::: aside
<https://youtu.be/ZXiruGOCn9s?si=jtJsAzNs8O5UDlwO>
:::

::: {style="text-align: center"}
## Questions? {.center}
:::

::: {style="text-align: center"}
## Bye! {.center}
:::