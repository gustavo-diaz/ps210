---
format:
    revealjs:
      slide-number: false
      progress: false
      chalkboard: true
      code-overflow: wrap
      theme: [default, custom.scss]
---

```{r setup, include=FALSE}
library(knitr)

opts_chunk$set(echo = FALSE, 
               message = FALSE, 
               warning = FALSE,
               out.width = "100%")

library(tidyverse)
library(tinytable)
library(kableExtra)
library(gt)
library(modelsummary)
library(tidyr)
library(broom)


# ggplot global options
theme_set(theme_gray(base_size = 20))

# set NU purple
nu_purple = "#4E2A84"

```



::: {style="text-align: center"}
## Large N {.center}

### POLI SCI 210

Introduction to Empirical Methods in Political Science

:::

## Be ready for next week! {.smaller background-color="#FDBF57"}

**NO EMPS CHAPTER ASSIGNED**

**Lecture reading**

- Huntinton-Klein, Nick. 2022. [*The Effect: An Introduction to Research Design and Causality*](https://theeffectbook.net). Chapman & Hall. Chapter 18

- Cattaneo, Matias D., Nicolás Idrobo, and Rocío Titiunik. 2020. [*A Practical Introduction to Regression Discontinuity Designs: Foundations*](https://doi.org/10.1017/9781108684606). Cambridge University Press. Chapters 1-4

**Discussion section (write critique about one only)**

- García-Montoya, Laura, Ana Arjona, and Matthew Lacombe. 2022. ["Violence and Voting in the United States: How School Shootings Affect Elections."](https://doi.org/10.1017/S0003055421001179) *American Political Science Review* 116 (3): 807-826
<!-- Critique: https://doi.org/10.1017/S0003055424000108 -->

- Saab, Andrew. 2026. ["Fickle loyalties: Intragroup competition in open list elections."](https://doi.org/10.1016/j.electstud.2025.103010) *Electoral Studies*


## Last week

- Experiments to learn about *cause and effect*

- **Broadly:** Summarizing relationships between two variables (difference in means)

- **This week:** A more general method to summarize relationships between two (or more) variables

- **Tuesday:** *Bivariate* relationships

- [**Thursday:**](#thu) *Multivariate* relationships

## An experiment has two variables

::: incremental
- **Y:** Observed outcome

- **D:** Treatment assignment (0: control, 1: treatment)
:::

. . . 

**Y** can be any kind of variable (numerical, categorical)

. . .

**D** is categorical because it denotes group membership 


## More general names

. . .

```{r}
variables = tribble(
  ~Y, ~X,
  "Outcome variable", "Explanatory variable",
  "Response variable", "Predictor variable",
  "Dependent variable", "Independent variable",
  "Thing you **_want_** to explain", "Thing you **_use_** to explain"
)
```

```{r}
variables %>% 
  tt() %>% 
  format_tt(i = 4, markdown = TRUE) %>% 
  style_tt(j = 1:2, align = "c") %>% 
  style_tt(i = 4, fontsize = 0.6) %>% 
  style_tt(j = 2, color = "white")
```

## More general names

```{r}
variables %>% 
  tt() %>% 
  format_tt(i = 4, markdown = TRUE) %>% 
  style_tt(j = 1:2, align = "c") %>% 
  style_tt(i = 4, fontsize = 0.6)
```


. . .

**X** and **Y** can now be any type (numerical, categorical)

. . .

That means we can't just compare means



## Example

```{r make-cookies, include=FALSE}
cookies <- tibble(happiness = c(0.5, 2, 1, 2.5, 3, 1.5, 2, 2.5, 2, 3),
                  cookies = 1:10)

cookies_data <- cookies
cookies_model <- lm(happiness ~ cookies, data = cookies)
cookies_fitted <- augment(cookies_model)
```

```{r cookies-base}
cookies_base <- ggplot(cookies_fitted, aes(x = cookies, y = happiness)) +
  geom_point(size = 3) +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 3)) +
  scale_x_continuous(breaks = 0:10) +
  labs(x = "Cookies eaten", y = "Level of happiness")

cookies_base
```

## How to summarize?

```{r}
cookies_base
```


## Connect with a line

```{r cookies-spline}
cookies_base +
  geom_smooth(method = lm, color = nu_purple, formula = y ~ splines::bs(x, 7), se = FALSE)
```

## Maybe smoother?

```{r cookies-loess}
cookies_base +
  geom_smooth(method = "loess", color = nu_purple, se = FALSE)
```

## A straight line?

```{r cookies-lm}
cookies_base +
  geom_smooth(method = "lm", color = nu_purple, se = FALSE)
```

## Straight lines are good

. . .

They can be written as a **linear equation**

. . .

$$
y = mx + b
$$

. . .

```{=html}
<table>
  <tr>
    <td class="cell-center">\(y\)</td>
    <td class="cell-left">&ensp;Outcome variable</td>
  </tr>
  <tr>
    <td class="cell-center">\(x\)</td>
    <td class="cell-left">&ensp;Explanatory variable</td>
  </tr>
  <tr>
    <td class="cell-center">\(m\)</td>
    <td class="cell-left">&ensp;Slope (\(\frac{\text{rise}}{\text{run}}\))</td>
  </tr>
  <tr>
    <td class="cell-center">\(b\)</td>
    <td class="cell-left">&ensp;y-intercept</td>
  </tr>
</table>
```

. . .

This is the *smallest number of parameters* to draw a line

## Slopes and intercepts

:::: {.columns}

::: {.column width="50%"}
$y = 2x - 1$

```{r simple-line-1, echo=FALSE, fig.dim=c(4.8, 3.5), out.width="100%"}
ggplot(data = tibble(x = 0:5), aes(x = x)) +
  stat_function(fun = function(x) 2 * x - 1, 
                color = nu_purple, size = 1.5) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  scale_x_continuous(breaks = 0:5) +
  scale_y_continuous(breaks = -1:9)
```
:::

:::

## Slopes and intercepts

:::: {.columns}

::: {.column width="50%"}
$y = 2x - 1$

```{r simple-line-1b, echo=FALSE, fig.dim=c(4.8, 3.5), out.width="100%"}
ggplot(data = tibble(x = 0:5), aes(x = x)) +
  stat_function(fun = function(x) 2 * x - 1, 
                color = nu_purple, size = 1.5) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  scale_x_continuous(breaks = 0:5) +
  scale_y_continuous(breaks = -1:9)
```
:::

::: {.column width="50%"}
$y = -0.5x + 6$

```{r simple-line-2, echo=FALSE, fig.dim=c(4.8, 3.5), out.width="100%"}
ggplot(data = tibble(x = 0:14), aes(x = x)) +
  stat_function(fun = function(x) -0.5 * x + 6, 
                color = nu_purple, size = 1.5) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  scale_x_continuous(breaks = 0:14) +
  scale_y_continuous(breaks = -1:9) +
  theme(panel.grid.minor = element_blank())
```
:::

:::

. . .

We can think of *intercept* and *slope* as **estimands** or **inferential targets**

## Drawing lines in statistics

. . .

$$
y = mx + b
$$

## Drawing lines in statistics

$$
\widehat{y} = \widehat{\beta}_0 + \widehat{\beta}_1 x_1
$$

. . .

```{=html}
<table>
  <tr>
    <td class="cell-center">\(y\)</td>
    <td class="cell-center">\(\widehat{y}\)</td>
    <td class="cell-left">&ensp;Outcome variable</td>
  </tr>
  <tr>
    <td class="cell-center">\(x\)</td>
    <td class="cell-center">\(x_1\)</td>
    <td class="cell-left">&ensp;Explanatory variable</td>
  </tr>
  <tr>
    <td class="cell-center">\(m\)</td>
    <td class="cell-center">\(\widehat{\beta}_1\)</td>
    <td class="cell-left">&ensp;Slope</td>
  </tr>
  <tr>
    <td class="cell-center">\(b\)</td>
    <td class="cell-center">\(\widehat{\beta}_0\)</td>
    <td class="cell-left">&ensp;y-intercept</td>
  </tr>
</table>
```

::: aside
You may see this equation with an extra error term $\varepsilon$ in some textbooks
:::

## What are we doing?

::: incremental
- **Before:** Assume there is a *true parameter* that we do not observe (e.g. population mean, ATE)

- **Now:** Assume there is a *true line* that best describes the relationship between X and Y

- There is a **best linear predictor** that we want to *estimate*
:::

## Which line is a better summary?

![](https://ellaudet.scholars.harvard.edu/sites/g/files/omnuum9971/files/styles/hwp_1_1__1440x1440_scale/public/ellaudet/files/the_least_squares_method_new.gif)

## More formally

::: incremental
- The **best linear predictor** is the line that minimizes the distance of each observation to the line

- That distance is known as **residual** or **error**
:::

## Visualizing residuals

```{r}
cookies_base +
  geom_smooth(method = "lm", color = nu_purple, se = FALSE)
```

## Visualizing residuals

```{r}
cookies_with_residual <- cookies_base +
  geom_smooth(method = "lm", color = nu_purple, se = FALSE) +
  geom_segment(aes(xend = cookies, yend = .fitted), color = "#FF851B", size = 1)

cookies_with_residual
```


## More formally

- The **best linear predictor** is the line that minimizes the distance of each observation to to the line

- That distance is know as a **residual** or **error**

. . .

$$
e_i = (y_i - \widehat y_i)
$$

## More formally

- The **best linear predictor** is the line that minimizes the distance of each observation to to the line

- That distance is know as a **residual** or **error**


$$
e_i = (y_i - (b_0 + b_1 x_{1i}))
$$

## Minimizing residuals

We want to find a **vector of coefficients** ($\widehat \beta_0$, $\widehat \beta_1$) that minimizes the **sum of squared residuals**

. . .

$$
SSR = \sum_{i=1}^n e_i^2
$$

. . .

We could try many lines until we find the the smallest SSR

. . .

Or use a method called **Ordinary Least Squares** (OLS)

## OLS regression

. . .

**Estimand**

$\alpha = E[Y] - \frac{\text{Cov}[X,Y]}{V[X]}E[X] \qquad \beta = \frac{\text{Cov}[X,Y]}{V[X]}$

&nbsp;

. . .

**Estimator**

$\widehat\alpha = \overline Y - \frac{\overline{XY} - \overline{X} \cdot \overline{Y}}{\overline{X^2} - \overline{X}^2} \overline{X} \qquad \widehat{\beta} = \frac{\overline{XY} - \overline{X} \cdot \overline{Y}}{\overline{X^2} - \overline{X}^2}$



::: aside
$\widehat{\alpha}$: intercept; $\widehat{\beta}$: slope
:::

## Back to cookies

. . .

$$
\widehat{y} = \widehat \beta_0 + \widehat \beta_1 x_1
$$

## Back to cookies

$$
\widehat{\text{happiness}} = \beta_0 + \beta
_1 \text{cookies}
$$

## Back to cookies

:::: {.columns}

::: {.column width="50%"}
$\widehat{\text{happiness}} = \beta_0 + \beta_1 \text{cookies}$
:::
::::

## Back to cookies

:::: {.columns}

::: {.column width="50%"}
$\widehat{\text{happiness}} = \beta_0 + \beta_1 \text{cookies}$

```{r fig.dim=c(4.8, 4.2), out.width="100%"}
cookies_base +
  geom_smooth(method = "lm", color = nu_purple) +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 4))
```
:::
::::

## Back to cookies

:::: {.columns}

::: {.column width="50%"}
$\widehat{\text{happiness}} = \beta_0 + \beta_1 \text{cookies}$

```{r fig.dim=c(4.8, 4.2), out.width="100%"}
cookies_base +
  geom_smooth(method = "lm", color = nu_purple) +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 4))
```
:::

::: {.column width="50%"}

```{r}
happiness_model <- 
  lm(happiness ~ cookies,
     data = cookies_data)

mod = modelsummary(list("happinness" = happiness_model),
             gof_map = c("nobs", "r.squared"),
             stars = TRUE,
             output = "kableExtra")

mod %>% 
  kable_styling(font_size = 25)
```


:::

::::

## Back to cookies

:::: {.columns}

::: {.column width="50%"}
$\widehat{\text{happiness}} = \beta_0 + \beta_1 \text{cookies}$

```{r fig.dim=c(4.8, 4.2), out.width="100%"}
cookies_base +
  geom_smooth(method = "lm", color = nu_purple) +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 4))
```
:::

::: {.column width="50%"}

```{r}
happiness_model <- 
  lm(happiness ~ cookies,
     data = cookies_data)

mod = modelsummary(list("happinness" = happiness_model),
             gof_map = c("nobs", "r.squared"),
             stars = c('*' = .05),
             output = "kableExtra")

mod %>% 
  kable_styling(font_size = 25)
```


:::

::::

## Back to cookies

:::: {.columns}

::: {.column width="50%"}
$\widehat{\text{happiness}} = 1.10 + 0.16 \cdot \text{cookies}$

```{r fig.dim=c(4.8, 4.2), out.width="100%"}
cookies_base +
  geom_smooth(method = "lm", color = nu_purple) +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 4))
```
:::

::: {.column width="50%"}

```{r}
happiness_model <- 
  lm(happiness ~ cookies,
     data = cookies_data)

mod = modelsummary(list("happinness" = happiness_model),
             gof_map = c("nobs", "r.squared"),
             stars = c('*' = .05),
             output = "kableExtra")

mod %>% 
  kable_styling(font_size = 25)
```


:::

::::

## Back to cookies

:::: {.columns}

::: {.column width="50%"}
$\widehat{\text{happiness}} = 1.10 + 0.16 \cdot \text{cookies}$

```{r fig.dim=c(4.8, 4.2), out.width="100%"}
cookies_base +
  geom_smooth(method = "lm", color = nu_purple) +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 4))
```
:::

::: {.column width="50%"}

```{r}
happiness_model <- 
  lm(happiness ~ cookies,
     data = cookies_data)

mod = modelsummary(list("happinness" = happiness_model),
             gof_map = c("nobs", "r.squared"),
             stars = c('*' = .05),
             output = "kableExtra")

mod %>% 
  kable_styling(font_size = 25)
```

**On average**

:::

::::

## Back to cookies

:::: {.columns}

::: {.column width="50%"}
$\widehat{\text{happiness}} = 1.10 + 0.16 \cdot \text{cookies}$

```{r fig.dim=c(4.8, 4.2), out.width="100%"}
cookies_base +
  geom_smooth(method = "lm", color = nu_purple) +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 4))
```
:::

::: {.column width="50%"}

```{r}
happiness_model <- 
  lm(happiness ~ cookies,
     data = cookies_data)

mod = modelsummary(list("happinness" = happiness_model),
             gof_map = c("nobs", "r.squared"),
             stars = c('*' = .05),
             output = "kableExtra")

mod %>% 
  kable_styling(font_size = 25)
```


**On average**, one additional cookie

:::

::::

## Back to cookies

:::: {.columns}

::: {.column width="50%"}
$\widehat{\text{happiness}} = 1.10 + 0.16 \cdot \text{cookies}$

```{r fig.dim=c(4.8, 4.2), out.width="100%"}
cookies_base +
  geom_smooth(method = "lm", color = nu_purple) +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 4))
```
:::

::: {.column width="50%"}

```{r}
happiness_model <- 
  lm(happiness ~ cookies,
     data = cookies_data)

mod = modelsummary(list("happinness" = happiness_model),
             gof_map = c("nobs", "r.squared"),
             stars = c('*' = .05),
             output = "kableExtra")

mod %>% 
  kable_styling(font_size = 25)
```


**On average**, one additional cookie increases happiness by 0.16 points

:::

::::

## Regression and correlation

. . .

Informally, we use regression coefficients (slopes) to determine whether two variables are **correlated**

. . .

Technically, they are related but on a different scale

. . .

**Regression coefficient:** $\beta = \frac{\text{Cov}[X,Y]}{V[X]}$

. . .

**Correlation:** $\rho = \frac{\text{Cov}[X,Y]}{SD[X] SD[Y]}$

::: aside
By the way, $\text{Cov}[X,Y] = E[(X-E[X])(Y-E[Y])]$
:::

## Regression and correlation

Informally, we use regression coefficients (slopes) to determine whether two variables are **correlated**

Technically, they are related but on a different scale

**Regression coefficient:** $\beta = \frac{\text{Cov}[X,Y]}{V[X]}$ $\Rightarrow$ in units of Y (happiness)

**Correlation:** $\rho = \frac{\text{Cov}[X,Y]}{SD[X] SD[Y]}$

::: aside
By the way, $\text{Cov}[X,Y] = E[(X-E[X])(Y-E[Y])]$
:::

## Regression and correlation

Informally, we use regression coefficients (slopes) to determine whether two variables are **correlated**

Technically, they are related but on a different scale

**Regression coefficient:** $\beta = \frac{\text{Cov}[X,Y]}{V[X]}$ $\Rightarrow$ in units of Y (happiness)

**Correlation:** $\rho = \frac{\text{Cov}[X,Y]}{SD[X] SD[Y]}$ $\Rightarrow$ [-1, 1] scale

::: aside
By the way, $\text{Cov}[X,Y] = E[(X-E[X])(Y-E[Y])]$
:::

## With cookies again

:::: {.columns}

::: {.column width="50%"}
$\widehat{\text{happiness}} = 1.10 + 0.16 \cdot \text{cookies}$

```{r fig.dim=c(4.8, 4.2), out.width="100%"}
cookies_base +
  geom_smooth(method = "lm", color = nu_purple) +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 4))
```
:::

::: {.column width="50%"}

&nbsp;

::: incremental

- **On average**, one additional cookie increases happiness by 0.16 points

- Corresponds to a correlation of `r round(cor(cookies$cookies, cookies$happiness), 2)`

:::

:::

::::

## Helpful for comparison

. . .

Is 0.16 happiness points per cookie a lot?

. . .

We cannot tell without a point of reference

. . .

But correlation is a reference on its own:

```{r, echo = FALSE}
correlation = tribble(
  ~`Absolute magnitude`, ~Effect,
  .1, "Small",
  .3, "Moderate",
  .5, "Large"
)

correlation %>% 
  tt()

```

## Summary

- Lines are a convenient way to summarize bivariate relationships

- We can treat line-fitting as an estimation problem

- OLS regression has good statistical properties (minimizes SSR)

- Regression and correlation are related but different

- Many different kinds of regression models!

::: {style="text-align: center"}
## Large N {#thu .center}

### POLI SCI 210

Introduction to Empirical Methods in Political Science

:::


```{r}
# remotes::install_github("jamesmartherus/anesr")
library(anesr)


data(timeseries_2016)

nes16 = timeseries_2016 %>% 
  select(
    V162079, # Feeling thermometer for TRUMP [POST]
    V162230x, # Better if man works and woman takes care of home [POST]
    V162255, # Is Barack Obama Muslim (yes/no) [POST]
    V161267, # Respondent Age [PRE]
    V161270, # Highest level of Education (Years) [PRE]
    V161158x # Party ID [PRE]
  )

nes16 = nes16 %>% 
  mutate(
    ft_trump_post = ifelse(V162079 < 0 | V162079 == 998, NA, V162079),
    women_at_home = case_when(V162230x < 0 ~ NA,
                              V162230x <= 3 ~ 1,
                              V162230x <= 7 ~ 0),
    obamamuslim = ifelse(V162255 == 1, 1, 0),
    age = ifelse(V161267 < 0, NA, V161267),
    age0 = age - 18,
    educ_hs = case_when(V161270 < 0 ~ NA,
                        V161270 >= 90 ~ NA,
                        V161270 >= 9 ~ 1,
                        V161270 <= 8 ~ 0),
    republican = case_when(V161158x < 0 ~ NA,
                           V161158x <= 4 ~ 0,
                           V161158x <= 7 ~ 1)
  )

nes = nes16 %>% 
  select(ft_trump_post,
         women_at_home, obamamuslim,
         age, age0,
         educ_hs, republican)

```


## Last time

::: incremental
- **Bivariate regression** as a method to understand relationship between X and Y

- **Today:** More variables! (and why you would want that)

- **Multivariate regression**

:::

## Running example: ANES 2016 data

. . .

**Outcome variable** 

- `ft_trump_post`: Post-election feeling thermometer toward Trump

. . .

```{r, out.width="80%", fig.align='center'}
ggplot(nes) +
  aes(x = ft_trump_post) +
  geom_histogram(bins = 10, fill = nu_purple)
```


## Running example: ANES 2016 data

**Explanatory variables**

::: incremental
- `women_at_home`: Believe women should stay home
- `obamamuslim`: Believe Obama is a Muslim
- `age`: Age in years
- `age0`: Age in years (starting with 18 = 0)
- `educ_hs`: Any kind of post-secondary education
- `republican`: Identifies with Republican party (including leaners)
:::

## Running example: ANES 2016 data


```{r, echo = FALSE}
nes
```

## Main relationship

```{r}
ggplot(nes) +
  aes(x = obamamuslim,
      y = ft_trump_post) +
  geom_jitter(size = 1, 
              width = 0.3,
              alpha = 0.8,
              color = nu_purple) +
  scale_x_continuous(breaks = c(0, 1))
```


## Regression as conditional means

. . .

$\widehat{\texttt{ft_trump_post}} = \beta_0 + \beta_1 \cdot \texttt{obamamuslim}$

. . .

```{r}
lm_race = lm(ft_trump_post ~ obamamuslim, data = nes)

lm_race %>% 
  tidy() %>% 
  select(term, estimate, std.error, p.value) %>% 
  kbl(digits = 2)
```

## Regression as conditional means

$\widehat{\texttt{ft_trump_post}} = 32.17 + 35.04 \cdot \texttt{obamamuslim}$

```{r}
lm_race = lm(ft_trump_post ~ obamamuslim, data = nes)

lm_race %>% 
  tidy() %>% 
  select(term, estimate, std.error, p.value) %>% 
  kbl(digits = 2)
```

::: incremental
- What is the average feeling thermometer for someone who *does not* believe Obama is a Muslim?

- What is the average feeling thermometer for someone who *does* believe Obama is a Muslim?
:::

## What can we say with regression?

::: incremental
- **Level 1:** Description of *conditional means*

- **Level 2:** Statistical inference (needs CIs or p-values)

- **Level 3:** Causal inference (needs **assumptions**)
:::

. . .

What do we need to assume to make causal claims?

::: aside
Berk, Richard. 2010. ["What You Can and Can't Properly do with Regression."](https://doi.org/10.1007/s10940-010-9116-4) *Journal of Quantitative Criminology* 26: 481-487
:::

## Strategy 1: Random assignment

. . .

If treatment $D$ is *randomly assigned*

::: incremental
- Potential outcomes are *independent* from treatment: $(Y(0), Y(1)) \perp \!\!\! \perp D$
- ATE $E[\tau_i]$ is **point-identified**
- Estimate with *difference in means* between treatment and control
- Bivariate regression yields the same result
:::

. . .

What if random assignment is not possible?

## Return to ANES 2016

. . .

We **found** that those who believe Obama is a Muslim were, on average, 35 points more favorable toward Trump

. . .

We want to **claim** this is because:

$$
\text{Beliefs about ethnicity} \Rightarrow \text{Support for Trump}
$$

. . .

What prevents us from making such a claim?

::: incremental
- Reverse causation
- Omitted variable bias
- Selection bias
:::

## Strategy 2: Ignorability


. . .

We want to be able to **ignore** the role of *potential confounders*

. . .

We usually do this by presenting a **controlled** comparison

. . .

So we can say that our *explanatory variable* is distributed in a way that is **conditionally independent**

. . .

**Conditional independence:** $(Y(0), Y(1)) \perp \!\!\! \perp D | \mathbf{X}$

. . .

We now distinguish between:

::: incremental
- *Explanatory* variable (D) 
- *Control* variables or *covariates* (X)
:::


::: aside
There is strong and weak ignorability. For our purposes they are the same
:::

## Another way to think about it

There *is* a causal effect to be found in **observational data**

. . .

But without random assignment, the effect is *contaminated* by potential confounders

. . .

We want to **adjust** or **control** for these variables

## Multivariate regression

. . .

The linear model setup is flexible

. . .

 $$\widehat{y} = \widehat \beta_0 + \widehat \beta_1 x_1 + \widehat \beta_2 x_2 + \ldots + \widehat \beta_K x_K$$

. . .

You can technically put *whatever* you want in a regression as long as $\text{observations} > \text{variables}$

. . .

But for *statistical* or *causal inference*, anything with more than 2-3 control variables doesn't make much sense

## Increasing dimensions

:::: {.columns}

::: {.column width="50%"}
![](fig/3d.webp)
:::

::::

## Increasing dimensions

:::: {.columns}

::: {.column width="50%"}
![](fig/3d.webp)
:::

::: {.column width="50%"}
More dimensions $\rightarrow$ more likely to see:

::: incremental
- **Extrapolation:** Fitting line beyond actual range

- **Interpolation:** Gaps within actual range

:::

:::

::::

. . .

Illusion of learning from **empty space**!

## Practice

. . .

We argue ethnicity beliefs $\Rightarrow$ support Trump

. . .

Some alternative explanations:

::: incremental
- Differences in education
- Differences in age
- Partisan motivated reasoning
:::

## Models

Estimate the following models:

. . .

$\widehat{\texttt{ft_trump_post}} = \beta_0 + \beta_1 \texttt{obamamuslim}$ (Baseline)

. . .

$\widehat{\texttt{ft_trump_post}} = \beta_0 + \beta_1 \texttt{obamamuslim} + \beta_2 \texttt{educ_hs}$

. . .

$\widehat{\texttt{ft_trump_post}} = \beta_0 + \beta_1 \texttt{obamamuslim} + \beta_2 \texttt{age}$

. . .

$\widehat{\texttt{ft_trump_post}} = \beta_0 + \beta_1 \texttt{obamamuslim} + \beta_2 \texttt{republican}$

. . .

$\widehat{\texttt{ft_trump_post}} = \beta_0 + \beta_1 \texttt{obamamuslim} + \beta_2  \texttt{educ_hs} +\\ \beta_3 \texttt{age} + \beta_4 \texttt{republican}$

## Results

```{r}
lm0 = lm(ft_trump_post ~ obamamuslim, data = nes)

lm1 = lm(ft_trump_post ~ obamamuslim + educ_hs, data = nes)

lm2 = lm(ft_trump_post ~ obamamuslim + age, data = nes)

lm3 = lm(ft_trump_post ~ obamamuslim + republican, data = nes)

lm4 = lm(ft_trump_post ~ obamamuslim + educ_hs +
           age + republican, data = nes)
```

```{r}
models = list(lm0, lm1, lm2, lm3, lm4)

mods = modelsummary(models,
             gof_map = c("nobs", "r.squared"),
             stars = c('*' = .05),
             output = "gt")

mods %>% 
  tab_style(
    style = cell_text(color = "#00000000"),
    locations = cells_body(columns = 2:6)) %>% 
    tab_style(
    style = cell_fill(color = "#B6ACD1"),
    locations = cells_body(rows = 3:4)
  )
```

## Results

```{r}
mods %>% 
  tab_style(
    style = cell_text(color = "#00000000"),
    locations = cells_body(columns = 3:6)) %>% 
  tab_style(
    style = cell_fill(color = "#B6ACD1"),
    locations = cells_body(rows = 3:4)
  )
```


## Results

```{r}
mods %>% 
  tab_style(
    style = cell_text(color = "#00000000"),
    locations = cells_body(columns = 4:6)) %>% 
    tab_style(
    style = cell_fill(color = "#B6ACD1"),
    locations = cells_body(rows = 3:4)
  )
```

## Results

```{r}
mods %>% 
  tab_style(
    style = cell_text(color = "#00000000"),
    locations = cells_body(columns = 5:6)) %>% 
    tab_style(
    style = cell_fill(color = "#B6ACD1"),
    locations = cells_body(rows = 3:4)
  )
```

## Results

```{r}
mods %>% 
  tab_style(
    style = cell_text(color = "#00000000"),
    locations = cells_body(columns = 6)) %>% 
    tab_style(
    style = cell_fill(color = "#B6ACD1"),
    locations = cells_body(rows = 3:4)
  )
```

## Results

```{r}
mods %>% 
    tab_style(
    style = cell_fill(color = "#B6ACD1"),
    locations = cells_body(rows = 3:4)
  )
```

## Visualizing

```{r}
names(models) = c("none",
                  "educ_hs",
                  "age",
                  "republican",
                  "all")

modelplot(models, coef_map = "obamamuslim") +
  labs(color = "Controls") +
  theme(legend.position = "bottom", 
        axis.text.y = element_text(
          angle = 90, hjust = 0.5)
        ) +
  geom_vline(xintercept = 0, linetype = "dashed")
```

## Everything else constant {.smaller}

Plug-in coefficients in equations:

. . .

$\widehat{\texttt{ft_trump_post}} = 32.17 + 35.04 \cdot \texttt{obamamuslim}$

. . .

$\widehat{\texttt{ft_trump_post}} =  32.76 + 34.85 \cdot \texttt{obamamuslim}  -0.56 \cdot \texttt{educ_hs}$

. . .

$\widehat{\texttt{ft_trump_post}} = 23.09 + 34.72 \cdot \texttt{obamamuslim} + 0.19 \cdot \texttt{age}$

. . .

$\widehat{\texttt{ft_trump_post}} = 20.367 + 22.74 \cdot \texttt{obamamuslim} + 37.53 \cdot \texttt{republican}$

. . .

$\widehat{\texttt{ft_trump_post}} = 21.34 + 22.23 \cdot \texttt{obamamuslim}  -6.22 \cdot  \texttt{educ_hs} +\\ 0.10 \cdot \texttt{age} + 37.54 \cdot \texttt{republican}$

. . .

&nbsp;

::: {.r-stack}
Coefficients now need to be interpreted as **marginal means** or **marginal slopes**
:::

. . .

&nbsp; 

::: {.r-stack}
These only make sense if you think *at least one variable* is a **focal point**
:::

## Interactions

. . .

What if we believed the effect of `obamamuslim` varies depending on attitudes about gender roles?

. . .

**Model:** $\widehat{\texttt{ft_trump_post}} = \beta_0 + \beta_1 \texttt{obamamuslim} + \beta_2 \texttt{women_at_home} + \\ \beta_3 \texttt{obamamuslim} \times \texttt{women_at_home}$

## Interactions

**Model:** $\widehat{\texttt{ft_trump_post}} = \beta_0 + \beta_1 \texttt{obamamuslim} + \beta_2 \texttt{women_at_home} + \\ \beta_3 \texttt{obamamuslim} \times \texttt{women_at_home}$

```{r}
lm_wah = lm(ft_trump_post ~ obamamuslim*women_at_home, data = nes)

tidy(lm_wah) %>% 
  mutate(term = ifelse(term == "obamamuslim:women_at_home", 
                       "interaction", term)) %>% 
  select(term, estimate, std.error, p.value) %>% 
  kbl(digits = 2)
```

## Summary

- Regression is a way to estimate conditional means

- Multivariate regression needs "everything else constant" interpretation

- Coefficients are now **marginal means** or **marginal slopes**

- Only makes sense from a causal inference perspective